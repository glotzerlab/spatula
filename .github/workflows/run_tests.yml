name: Run Unit Tests

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

on:
  # trigger on pull requests
  pull_request:

  # trigger on all commits to trunk branches
  push:
    branches:
      - "main"

  # trigger on request
  workflow_dispatch:

jobs:
  run_tests:
    name: Unit test [py${{ matrix.python }} ${{ matrix.os }}]
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-24.04]
        python: ['3.12', '3.13', '3.14']
        include:
          - os: 'macos-15'
            python: '3.14'
          - os: 'windows-2025'
            python: '3.14'

    steps:
      - name: Checkout Code
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          submodules: true
      - name: Set up Python
        uses: actions/setup-python@e797f83bcb11b83ae66e0230d6156d7c80228e7c # v6.0.0
        with:
          python-version: ${{ matrix.python }}
      - name: Set up Python environment
        uses: glotzerlab/workflows/setup-uv@894d9678188a02b06560fdeb2b9ba98d016cd371 # 0.10.0
        with:
          lockfile: ".github/workflows/requirements-test.txt"
      - name: Build and Install
        run: |
          uv pip install . --no-build-isolation --no-deps --system -v
      - name: Run Tests (Unix/macOS)
        if: runner.os != 'Windows'
        run: pytest tests/ -v
      - name: Run Tests (Windows)
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          pytest tests/ test_representations.py -v --junitxml=pytest-report.xml
          $code = $LASTEXITCODE

          # If everything passed, we're done.
          if ($code -eq 0) { exit 0 }

          # Only consider overriding for "tests failed" exit code.
          if ($code -eq 1 -and (Test-Path pytest-report.xml)) {
            [xml]$j = Get-Content pytest-report.xml

            # Handle both <testsuite> and <testsuites><testsuite/></testsuites>
            $suites = @()
            if ($j.testsuite) { $suites = @($j.testsuite) }
            elseif ($j.testsuites) { $suites = @($j.testsuites.testsuite) }

            $tests    = ($suites | % { [int]$_.tests }    | Measure-Object -Sum).Sum
            $failures = ($suites | % { [int]$_.failures } | Measure-Object -Sum).Sum
            $errors   = ($suites | % { [int]$_.errors }   | Measure-Object -Sum).Sum

            Write-Host ("first JUnit aggregate: tests={0} failures={1} errors={2}" -f $tests, $failures, $errors)

            $failures = ($suites | ForEach-Object { [int]$_.failures } | Measure-Object -Sum).Sum
            $errors   = ($suites | ForEach-Object { [int]$_.errors }   | Measure-Object -Sum).Sum
            $tests    = ($suites | ForEach-Object { [int]$_.tests }    | Measure-Object -Sum).Sum

            Write-Host ("second JUnit aggregate: tests={0} failures={1} errors={2}" -f $tests, $failures, $errors)

            # Only "forgive" if junit says we actually ran tests and none failed/errored.
            if ($tests -gt 0 -and $failures -eq 0 -and $errors -eq 0) {
              Write-Host "Non-zero exit but JUnit shows 0 failures and 0 errors across $tests tests. Treating as flaky reruns."
              exit 0
            }
          }

          # In every other case, return the original pytest exit code.
          exit $code

  tests_complete:
    name: All tests
    if: always()
    needs: [run_tests]
    runs-on: ubuntu-latest

    steps:
    - run: jq --exit-status 'all(.result == "success")' <<< '${{ toJson(needs) }}'
    - name: Done
      run: echo "Done."
